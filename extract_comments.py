#!/usr/bin/env python3
"""
æ²ç¤ºæ¿ã¾ã¨ã‚ã‚µã‚¤ãƒˆã‹ã‚‰ã‚³ãƒ¡ãƒ³ãƒˆã‚’æŠ½å‡ºã—ã¦YouTubeå°æœ¬å½¢å¼ã®ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«å¤‰æ›ã™ã‚‹ãƒ„ãƒ¼ãƒ«
"""

import re
import csv
import sys
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from urllib.parse import urlparse
from datetime import datetime

import click
import requests
from bs4 import BeautifulSoup
from tqdm import tqdm


class CommentExtractor:
    """æ²ç¤ºæ¿ã¾ã¨ã‚ã‚µã‚¤ãƒˆã‹ã‚‰ã‚³ãƒ¡ãƒ³ãƒˆã‚’æŠ½å‡ºã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, url: str):
        self.url = url
        self.domain = urlparse(url).netloc
        self.comments: List[Dict[str, str]] = []
        
    def fetch_html(self) -> Optional[str]:
        """HTMLã‚’å–å¾—"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            response = requests.get(self.url, headers=headers, timeout=30)
            response.raise_for_status()
            response.encoding = response.apparent_encoding
            return response.text
        except Exception as e:
            click.echo(f"âŒ HTMLå–å¾—ã‚¨ãƒ©ãƒ¼: {e}", err=True)
            return None
    
    def extract_comments(self, html: str) -> List[Dict[str, str]]:
        """HTMLã‹ã‚‰ã‚³ãƒ¡ãƒ³ãƒˆã‚’æŠ½å‡º"""
        soup = BeautifulSoup(html, 'html.parser')
        comments = []
        seen_texts = set()  # é‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨
        
        # ã‚ã«ã¾ã‚“æ²ç¤ºæ¿å°‚ç”¨ã®æŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯
        if 'animanch.com' in self.domain:
            return self._extract_animanch_comments(soup, seen_texts)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: ã‚³ãƒ¡ãƒ³ãƒˆç•ªå·ï¼ˆ>>1, >>2ãªã©ï¼‰ã‚’å«ã‚€è¦ç´ ã‚’æ¤œç´¢
        numbered_pattern = re.compile(r'>>\d+')
        
        # ã‚³ãƒ¡ãƒ³ãƒˆç•ªå·ã‚’å«ã‚€ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ¼ãƒ‰ã‚’æ¤œç´¢
        for text_node in soup.find_all(string=numbered_pattern):
            parent = text_node.find_parent(['div', 'p', 'li', 'article', 'section'])
            if parent:
                text = parent.get_text(separator=' ', strip=True)
                if text and len(text) > 10 and text not in seen_texts:
                    seen_texts.add(text)
                    number_match = re.search(r'>>(\d+)', text)
                    comment_number = number_match.group(1) if number_match else None
                    
                    comments.append({
                        'number': comment_number or str(len(comments) + 1),
                        'speaker': self._extract_speaker(parent, text),
                        'text': text
                    })
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: ã‚¯ãƒ©ã‚¹åã‚„IDã«ã‚³ãƒ¡ãƒ³ãƒˆé–¢é€£ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€è¦ç´ 
        comment_keywords = ['comment', 'res', 'post', 'message', 'reply', 'response']
        for keyword in comment_keywords:
            # ã‚¯ãƒ©ã‚¹åã§æ¤œç´¢
            elements = soup.find_all(['div', 'p', 'li', 'article'], 
                                   class_=re.compile(keyword, re.I))
            for elem in elements:
                text = elem.get_text(separator=' ', strip=True)
                if text and len(text) > 10 and text not in seen_texts:
                    seen_texts.add(text)
                    number_match = re.search(r'>>(\d+)', text)
                    comment_number = number_match.group(1) if number_match else None
                    
                    comments.append({
                        'number': comment_number or str(len(comments) + 1),
                        'speaker': self._extract_speaker(elem, text),
                        'text': text
                    })
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³3: IDå±æ€§ã§æ¤œç´¢
        for elem in soup.find_all(['div', 'p', 'li'], id=re.compile(r'res|comment|post', re.I)):
            text = elem.get_text(separator=' ', strip=True)
            if text and len(text) > 10 and text not in seen_texts:
                seen_texts.add(text)
                number_match = re.search(r'>>(\d+)', text)
                comment_number = number_match.group(1) if number_match else None
                
                comments.append({
                    'number': comment_number or str(len(comments) + 1),
                    'speaker': self._extract_speaker(elem, text),
                    'text': text
                })
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³4: é•·ã„ãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã‚€è¦ç´ ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        if not comments:
            click.echo("âš ï¸  æ¨™æº–ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã‚³ãƒ¡ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢ã‚’å®Ÿè¡Œã—ã¾ã™...")
            for elem in soup.find_all(['div', 'p', 'article']):
                text = elem.get_text(separator=' ', strip=True)
                # 20æ–‡å­—ä»¥ä¸Šã§ã€æ”¹è¡Œã‚„å¥èª­ç‚¹ã‚’å«ã‚€ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚³ãƒ¡ãƒ³ãƒˆå€™è£œã¨ã™ã‚‹
                if (text and len(text) >= 20 and 
                    text not in seen_texts and
                    ('ã€‚' in text or 'ï¼' in text or 'ï¼Ÿ' in text or '\n' in text)):
                    seen_texts.add(text)
                    number_match = re.search(r'>>(\d+)', text)
                    comment_number = number_match.group(1) if number_match else None
                    
                    comments.append({
                        'number': comment_number or str(len(comments) + 1),
                        'speaker': self._extract_speaker(elem, text),
                        'text': text
                    })
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§ã¯æœ€å¤§50ä»¶ã¾ã§
                    if len(comments) >= 50:
                        break
        
        # ã‚³ãƒ¡ãƒ³ãƒˆç•ªå·ã§ã‚½ãƒ¼ãƒˆï¼ˆç•ªå·ãŒã‚ã‚‹å ´åˆï¼‰
        comments_with_number = [c for c in comments if c['number'].isdigit()]
        comments_without_number = [c for c in comments if not c['number'].isdigit()]
        
        if comments_with_number:
            comments_with_number.sort(key=lambda x: int(x['number']))
        
        return comments_with_number + comments_without_number
    
    def _extract_animanch_comments(self, soup: BeautifulSoup, seen_texts: set) -> List[Dict[str, str]]:
        """ã‚ã«ã¾ã‚“æ²ç¤ºæ¿å°‚ç”¨ã®ã‚³ãƒ¡ãƒ³ãƒˆæŠ½å‡º"""
        comments = []
        
        # IDãŒresã§å§‹ã¾ã‚‹è¦ç´ ã‚’æ¤œç´¢ï¼ˆres1, res2, res3ãªã©ï¼‰
        for elem in soup.find_all(id=re.compile(r'^res\d+$')):
            # ãƒ•ã‚©ãƒ¼ãƒ éƒ¨åˆ†ã‚’é™¤å¤–
            if 'resform' in str(elem.get('class', [])):
                continue
            
            text = elem.get_text(separator=' ', strip=True)
            if not text or len(text) < 10:
                continue
            
            # IDã‹ã‚‰ãƒ¬ã‚¹ç•ªå·ã‚’å–å¾—ï¼ˆres1 -> 1ï¼‰
            res_id = elem.get('id', '')
            res_number_match = re.search(r'res(\d+)', res_id)
            if not res_number_match:
                continue
            res_number = res_number_match.group(1)
            
            # ç™ºè¨€è€…åã¨ã‚³ãƒ¡ãƒ³ãƒˆå†…å®¹ã‚’æŠ½å‡º
            # ãƒ‘ã‚¿ãƒ¼ãƒ³: "1ã‚¹ãƒ¬ä¸»25/11/29(åœŸ) 15:39:202å ±å‘Š"ã‚³ãƒ¡ãƒ³ãƒˆå†…å®¹"
            speaker_match = re.search(r'^\d+([^0-9]+?)\d{2}/\d{2}/\d{2}', text)
            if speaker_match:
                speaker_raw = speaker_match.group(1).strip()
                speaker = self._clean_animanch_speaker(speaker_raw)
            else:
                speaker = "åŒ¿å"
            
            # ã‚³ãƒ¡ãƒ³ãƒˆå†…å®¹ã‚’æŠ½å‡ºï¼ˆæ—¥ä»˜æƒ…å ±ã®å¾Œã‹ã‚‰ï¼‰
            # ã€Œå ±å‘Šã€ã®å¾Œã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
            report_match = re.search(r'å ±å‘Š(.+)$', text)
            if report_match:
                comment_text = report_match.group(1).strip()
            else:
                # ã€Œå ±å‘Šã€ãŒãªã„å ´åˆã¯ã€æ—¥ä»˜ã®å¾Œã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
                date_match = re.search(r'\d{2}:\d{2}:\d{2}(.+)$', text)
                if date_match:
                    comment_text = date_match.group(1).strip()
                else:
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æœ€åˆã®æ•°å­—ã¨æ—¥ä»˜ã‚’é™¤ã„ãŸéƒ¨åˆ†
                    comment_text = re.sub(r'^\d+[^\d]+?\d{2}/\d{2}/\d{2}[^"]*', '', text).strip()
            
            # ç©ºã®ã‚³ãƒ¡ãƒ³ãƒˆã¯ã‚¹ã‚­ãƒƒãƒ—
            if not comment_text or len(comment_text) < 5:
                continue
            
            # é‡è¤‡ãƒã‚§ãƒƒã‚¯
            comment_key = f"{res_number}:{comment_text[:50]}"
            if comment_key in seen_texts:
                continue
            seen_texts.add(comment_key)
            
            comments.append({
                'number': res_number,
                'speaker': speaker,
                'text': comment_text
            })
        
        # ãƒ¬ã‚¹ç•ªå·ã§ã‚½ãƒ¼ãƒˆ
        comments.sort(key=lambda x: int(x['number']) if x['number'].isdigit() else 9999)
        return comments
    
    def _clean_animanch_speaker(self, speaker_raw: str) -> str:
        """ã‚ã«ã¾ã‚“æ²ç¤ºæ¿ã®ç™ºè¨€è€…åã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        # æ—¥ä»˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é™¤å»ï¼ˆ25/11/29(åœŸ) 15:39:20ãªã©ï¼‰
        speaker = re.sub(r'\d{2}/\d{2}/\d{2}\(.*?\)\s+\d{2}:\d{2}:\d{2}', '', speaker_raw)
        speaker = speaker.strip()
        
        # ç©ºã®å ´åˆã¯ã€ŒåŒ¿åã€
        if not speaker:
            return "åŒ¿å"
        
        return speaker
    
    def _extract_speaker(self, element, text: str) -> str:
        """ç™ºè¨€è€…åã‚’æŠ½å‡º"""
        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: IDã‚„åå‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆï¼ˆã‚ˆã‚Šè©³ç´°ãªãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
        id_patterns = [
            r'ID[ï¼š:]\s*([^\s\nï¼š:]+)',
            r'åå‰[ï¼š:]\s*([^\s\nï¼š:]+)',
            r'æŠ•ç¨¿è€…[ï¼š:]\s*([^\s\nï¼š:]+)',
            r'^([^ï¼š:\n]+)[ï¼š:]',  # è¡Œé ­ã®ã€Œåå‰ï¼šã€ãƒ‘ã‚¿ãƒ¼ãƒ³
            r'(\w+)[ï¼š:]\s*',  # ä¸€èˆ¬çš„ãªã€Œåå‰ï¼šã€ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆæœ€å¾Œã«è©¦è¡Œï¼‰
        ]
        
        for pattern in id_patterns:
            match = re.search(pattern, text)
            if match:
                speaker = match.group(1).strip()
                # çŸ­ã™ãã‚‹ã‚‚ã®ã‚„æ•°å­—ã®ã¿ã¯é™¤å¤–
                if len(speaker) > 1 and not speaker.isdigit():
                    return speaker
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: è¦ªè¦ç´ ã‹ã‚‰ã‚¯ãƒ©ã‚¹åã‚„IDã‚’å–å¾—
        if hasattr(element, 'get'):
            # ã‚¯ãƒ©ã‚¹åã‹ã‚‰æŠ½å‡º
            speaker_class = element.get('class', [])
            if speaker_class:
                for cls in speaker_class:
                    cls_lower = cls.lower()
                    if any(keyword in cls_lower for keyword in ['name', 'user', 'id', 'author', 'poster']):
                        # ã‚¯ãƒ©ã‚¹åã‹ã‚‰å®Ÿéš›ã®åå‰ã‚’æŠ½å‡º
                        name_match = re.search(r'([a-zA-Z0-9_]+)', str(cls))
                        if name_match:
                            return name_match.group(1)
            
            # IDå±æ€§ã‹ã‚‰æŠ½å‡º
            speaker_id = element.get('id', '')
            if speaker_id:
                id_lower = speaker_id.lower()
                if any(keyword in id_lower for keyword in ['name', 'user', 'id', 'author', 'poster']):
                    name_match = re.search(r'([a-zA-Z0-9_]+)', speaker_id)
                    if name_match:
                        return name_match.group(1)
            
            # dataå±æ€§ã‹ã‚‰æŠ½å‡º
            for attr_name, attr_value in element.attrs.items():
                if any(keyword in attr_name.lower() for keyword in ['name', 'user', 'id', 'author']):
                    if isinstance(attr_value, str) and len(attr_value) > 1:
                        return attr_value
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³3: ãƒ†ã‚­ã‚¹ãƒˆå†…ã®æœ€åˆã®è¡Œã‹ã‚‰åå‰ã‚’æ¨æ¸¬
        first_line = text.split('\n')[0].split('ï¼š')[0].split(':')[0].strip()
        if len(first_line) > 1 and len(first_line) < 30 and not first_line.startswith('>>'):
            return first_line
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³4: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ã€ŒåŒ¿åã€
        return "åŒ¿å"
    
    def convert_to_script_format(self, comments: List[Dict[str, str]]) -> List[Tuple[str, str]]:
        """ã‚³ãƒ¡ãƒ³ãƒˆã‚’å°æœ¬å½¢å¼ã«å¤‰æ›"""
        script_lines = []
        
        for comment in comments:
            speaker = comment['speaker']
            text = comment['text']
            
            # ãƒ†ã‚­ã‚¹ãƒˆã‚’é©åˆ‡ãªé•·ã•ã«åˆ†å‰²ï¼ˆ20-30å­—ç¨‹åº¦ã€æœ€å¤§50å­—ï¼‰
            lines = self._split_text(text, max_length=50)
            
            for line in lines:
                script_lines.append((speaker, line))
        
        return script_lines
    
    def _split_text(self, text: str, max_length: int = 50) -> List[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’é©åˆ‡ãªé•·ã•ã«åˆ†å‰²"""
        if len(text) <= max_length:
            return [text]
        
        lines = []
        current_line = ""
        
        # å¥èª­ç‚¹ã‚„æ”¹è¡Œã§åˆ†å‰²
        sentences = re.split(r'([ã€‚ï¼ï¼Ÿ\n])', text)
        
        for i in range(0, len(sentences), 2):
            sentence = sentences[i] + (sentences[i+1] if i+1 < len(sentences) else '')
            
            if len(current_line) + len(sentence) <= max_length:
                current_line += sentence
            else:
                if current_line:
                    lines.append(current_line.strip())
                current_line = sentence
        
        if current_line:
            lines.append(current_line.strip())
        
        return lines if lines else [text[:max_length]]
    
    def save_to_spreadsheet(self, script_lines: List[Tuple[str, str]], output_path: Path, format_type: str = 'tsv'):
        """ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆå½¢å¼ï¼ˆTSV/CSVï¼‰ã§ä¿å­˜"""
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        delimiter = '\t' if format_type == 'tsv' else ','
        
        with open(output_path, 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f, delimiter=delimiter, lineterminator='\n')
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œ
            writer.writerow(['ç™ºè¨€è€…', 'ã‚»ãƒªãƒ•'])
            
            # ãƒ‡ãƒ¼ã‚¿è¡Œ
            for speaker, line in script_lines:
                writer.writerow([speaker, line])
        
        click.echo(f"âœ… ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")


@click.command()
@click.argument('url', type=str)
@click.option('--output', '-o', type=click.Path(), default=None,
              help='å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: output/comments_YYYYMMDD_HHMMSS.tsvï¼‰')
@click.option('--format', 'output_format', type=click.Choice(['tsv', 'csv']), default='tsv',
              help='å‡ºåŠ›å½¢å¼ï¼ˆtsv ã¾ãŸã¯ csvã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: tsvï¼‰')
def main(url: str, output: Optional[str], output_format: str):
    """
    æ²ç¤ºæ¿ã¾ã¨ã‚ã‚µã‚¤ãƒˆã‹ã‚‰ã‚³ãƒ¡ãƒ³ãƒˆã‚’æŠ½å‡ºã—ã¦YouTubeå°æœ¬å½¢å¼ã®ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«å¤‰æ›
    
    URL: æ²ç¤ºæ¿ã¾ã¨ã‚ã‚µã‚¤ãƒˆã®URL
    """
    click.echo("ğŸš€ ã‚³ãƒ¡ãƒ³ãƒˆæŠ½å‡ºã‚’é–‹å§‹ã—ã¾ã™...")
    
    # å‡ºåŠ›ãƒ‘ã‚¹ã®è¨­å®š
    if output:
        output_path = Path(output)
    else:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = Path("output")
        output_dir.mkdir(exist_ok=True)
        extension = output_format
        output_path = output_dir / f"comments_{timestamp}.{extension}"
    
    # ã‚³ãƒ¡ãƒ³ãƒˆæŠ½å‡º
    extractor = CommentExtractor(url)
    
    click.echo(f"ğŸ“¥ HTMLã‚’å–å¾—ä¸­: {url}")
    html = extractor.fetch_html()
    
    if not html:
        click.echo("âŒ HTMLã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ", err=True)
        sys.exit(1)
    
    click.echo("ğŸ” ã‚³ãƒ¡ãƒ³ãƒˆã‚’æŠ½å‡ºä¸­...")
    comments = extractor.extract_comments(html)
    
    if not comments:
        click.echo("âš ï¸  ã‚³ãƒ¡ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ", err=True)
        click.echo("ğŸ’¡ ãƒ’ãƒ³ãƒˆ: ã‚µã‚¤ãƒˆã®æ§‹é€ ãŒæƒ³å®šã¨ç•°ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
        sys.exit(1)
    
    click.echo(f"âœ… {len(comments)}ä»¶ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’æŠ½å‡ºã—ã¾ã—ãŸ")
    
    # å°æœ¬å½¢å¼ã«å¤‰æ›
    click.echo("ğŸ“ å°æœ¬å½¢å¼ã«å¤‰æ›ä¸­...")
    script_lines = extractor.convert_to_script_format(comments)
    
    # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«ä¿å­˜
    click.echo("ğŸ’¾ ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«ä¿å­˜ä¸­...")
    extractor.save_to_spreadsheet(script_lines, output_path, output_format)
    
    click.echo(f"\nâœ¨ å®Œäº†ï¼ {len(script_lines)}è¡Œã®å°æœ¬ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¾ã—ãŸ")
    click.echo(f"ğŸ“Š ãƒ•ã‚¡ã‚¤ãƒ«: {output_path.absolute()}")


if __name__ == '__main__':
    main()

